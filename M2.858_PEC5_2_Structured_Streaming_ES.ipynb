{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Francisco Javier Morales Hidalgo\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7999b5da34520ecf6bb63d1eda26bfd5",
     "grade": false,
     "grade_id": "logos",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)  ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c82856d1ced38213fc3860b2f9d6ecbc",
     "grade": false,
     "grade_id": "intro_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# PEC_5_2: Structured Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "123e7730f23a4a19be4280a802c891b8",
     "grade": false,
     "grade_id": "intro_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En esta PEC vamos a trabajar con [Spark Structured Streaming](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html), un motor de procesamiento de flujo escalable y tolerante a fallos construido sobre el motor Spark SQL. \n",
    "\n",
    "Spark Structured Streaming nos permite realizar nuestro análisis de datos en streaming de la misma manera que lo hacemos con el procesamiento por lotes sobre datos estáticos. Ahora bien, hay que tener en cuenta que el structured streaming tiene una serie de ventajas. Por ejemplo, que el motor Spark SQL se encargará de ejecutar los analísis programados de forma incremental y continua, generando el resultado final a medida que datos de transmisión. Spark Streaming se basa en la API de Dataset/DataFrame que se puede utilizar Scala, Java, Python o R para expresar agregaciones de transmisión, ventanas de tiempo de eventos, etc. Finalmente, el sistema asegura garantías de tolerancia a fallos de un extremo a otro a través de puntos de control y registros de escritura anticipada.\n",
    "\n",
    "\n",
    "**IMPORTANTE: Para realizar esta práctica debes hacerlo mediante SSH desde terminal o VSCODE, y poner el código de la misma en este NOTEBOOK solo para su corrección.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0edfcd16d7cc2be22efd3561ff651ef",
     "grade": false,
     "grade_id": "entrega",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### entrega: \n",
    "El formato de entrega será un directorio comprimido en formato gnuzip bajo el nombre `PEC5_username.tar.gz`, substituyendo username por vuestro\tnombre\tde\tusuario. El contenido debe ser un fichero para cada programa Python por cada ejercicio indicando el apartado de los notebooks de enunciado, por ejemplo `PEC5_username_2_1_4.py` .Adjuntar los dos notebooks de la PEC, con el nombre `PEC5_1_username`, y `PEC5_2_username` con las salidas obtenidas que se piden, y las respuestas a las preguntas conceptuales planteadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87ffdb73f2d251d84399ec3c19e1abd2",
     "grade": false,
     "grade_id": "intro_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. PARTE 1. Word Count con Structured Streaming\n",
    "1. PARTE 2. Operaciones de ventana sobre eventos temporales\n",
    "1. PARTE 3. Captura y procesamiento de datos en tiempo real de la API OpenSky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "531165577e5611b2e2f75fa2041d63e9",
     "grade": false,
     "grade_id": "intro_1_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## PARTE 1. Word Count con Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ada6b9be4e906fd0e2247dd27cd76c20",
     "grade": false,
     "grade_id": "intro_1_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En esta primera parte de la PEC vamos a ver como implementar un word count conectando por sockets mediante un proceso netcat https://en.wikipedia.org/wiki/Netcat corriendo en una terminal vía SSH o VSCode y donde vais a ir escribiendo palabras, que posteriormente van a ser contadas.\n",
    "\n",
    "Para empezar, vamos a realizar un primer ejercicio guiado donde vamos a contar las palabras haciendo uso de los DataFrames que nos ofrece Structured Streaming. \n",
    "\n",
    "La siguiente celda de Jupyter Notebook crea un objeto spark que corresponde a una instancia de SparkSession. En las versiones modernas de Spark, la clase SparkSession es el punto de entrada a una aplicación Spark para cualquier tipo de Spark API (RDD, SparkSQL, Streaming, etc). Se pide ejecutar la siguiente celda y comprobar que se ha ejecutado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcd7af8becd4d1ded76f3a136c354125",
     "grade": false,
     "grade_id": "intro_1_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0-cdh6.2.0\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "# Introducid el nombre de la app PEC5_ seguido de vuestro nombre de usuario\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_fmoralesh\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0ece4b688f5dc795f8672c708863bbb",
     "grade": false,
     "grade_id": "intro_1_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Mediante el objeto spark vamos a configurar una lectura de datos en streaming reportados en el puerto que tenéis asociado, dado que es donde está el netcat estará funcionando. En el código de la siguiente celda debéis cambiar \\<PUERTO_ASIGNADO\\> por vuestro puerto.\n",
    "\n",
    "El DataFrame `linesDF` representa una tabla ilimitada que contiene la transmisión de datos de texto. Esta tabla contiene una columna de cadenas denominada `value`, y cada línea de los datos de texto de transmisión se convierte en una fila de la tabla. Tened en cuenta que todavía no está recibiendo ningún dato ya que solo estamos configurando la transformación y aún no hemos comenzado a recibir datos. Se pide al estudiante leer el código con detalle, revisar que se entienden todas las operaciones (consultar documentación en caso necesario) y ejecutar la celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c20d15f5fc553ea44d16cd7260753bb0",
     "grade": false,
     "grade_id": "intro_1_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Creamos el DataFrame representando el streaming de las lineas que nos entran por host:port\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', 'localhost')\\\n",
    "    .option('port', 20036)\\\n",
    "    .load()\n",
    "\n",
    "# Separamos las lineas en palabras en un nuevo DF\n",
    "#las funciones explode y split estan explicadas en\n",
    "#https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html\n",
    "wordsDF = linesDF.select(\n",
    "    explode(\n",
    "        split(linesDF.value, ' ')\n",
    "    ).alias('palabra')\n",
    ")\n",
    "\n",
    "# Generamos el word count en tiempo de ejecución\n",
    "wordCountsDF = wordsDF.groupBy('palabra').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e7c05dd69b313e06b5de126d2ad9691",
     "grade": false,
     "grade_id": "intro_1_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora que hemos configurado la consulta (análisis) sobre los datos de transmisión, declaramos la consulta para comenzar a recibir los datos y contar las palabras. Para hacer esto, vamos a configurar la salida del análisis para que imprima el conjunto completo de recuentos, especificado por `outputMode(\"complete\")` y configurado para trabajar en memoria cada vez que se actualizan. Finalmente iniciamos el cálculo de streaming usando `start()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df2f00811f41f1f183c003c14167cf1b",
     "grade": false,
     "grade_id": "intro_1_7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Iniciamos la consuta que muestra por consola o almacena en memoria el word count. \n",
    "# Trabajamos a partir del DataFrame que contiene la agrupación de las palabras y el numero de repeticiones\n",
    "# Utilizamos el formato memory para poder mostrarlo en Notebook, \n",
    "#si ejecutamos en consola debemos poner el formato console\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .start()\n",
    "\n",
    "#en una ejecución desde el terminal de sistema, necesitamos evitar que el programa finalice mientras \n",
    "#se está ejecutando la consulta en un Thread separado y en segundo plano. \n",
    "#query.awaitTermination() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ab82a1c6e259e56bcc9a5127a17c139",
     "grade": false,
     "grade_id": "intro_1_8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En una **sesión de terminal mediante SSH, no mediante Jupyter terminal** debéis ejecutar un netcat `$ nc -lk <puerto_asignado>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29e169e6f6e19680038080db5fb3c6d7",
     "grade": false,
     "grade_id": "intro_1_10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Mediante esta celda podemos mostrar en el Notebook los datos de la consulta a la tabla `palabras` en una celda, y vamos actualizando esta celda cada 5 segundos. En este caso utilizamos una sentencia SQL. Como se trata de un bucle sobre el Notebook deberéis parar el kernel una vez vista la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89e3b3e2150f43ece760da72511cd267",
     "grade": false,
     "grade_id": "intro_1_11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'isDataAvailable': True,\n",
       " 'isTriggerActive': True,\n",
       " 'message': 'Processing new data'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|palabra|count|\n",
      "+-------+-----+\n",
      "|   Hola|    1|\n",
      "|      a|    4|\n",
      "|    ASD|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e14d56bae364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT * FROM palabras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT * FROM palabras').show())\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e1b0ea10c287fbb0da25369ffafdbe1",
     "grade": false,
     "grade_id": "out_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "`{'isDataAvailable': False,\n",
    " 'isTriggerActive': True,\n",
    " 'message': 'Waiting for data to arrive'}\n",
    "+-------+-----+\n",
    "|palabra|count|\n",
    "+-------+-----+\n",
    "|   Data|    2|\n",
    "|    UOC|    2|\n",
    "|    Big|    2|\n",
    "|  Spark|    1|\n",
    "+-------+-----+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "245ef74107308b5b0f13e50336ffc1e8",
     "grade": false,
     "grade_id": "intro_1_8_b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Alternativamente podemos consultar los datos que estamos recibiendo por streaming mediante la tabla `palabras`, pero tendremos que actualizar manualmente el show(). Tarda un tiempo en aparecer la primera salida en Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e11fe2a1d5254717127c43d3df009008",
     "grade": false,
     "grade_id": "intro_1_9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "| palabra|count|\n",
      "+--------+-----+\n",
      "|    Hola|    2|\n",
      "| artista|    1|\n",
      "|      er|    1|\n",
      "|Caracola|    1|\n",
      "|   vamor|    1|\n",
      "|     Que|    1|\n",
      "|     tio|    1|\n",
      "|  mostro|    1|\n",
      "|       a|    4|\n",
      "|    dise|    1|\n",
      "|     ASD|    3|\n",
      "|    Como|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"palabras\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e6f9eabdab9e25b12f90421cb0896ee",
     "grade": false,
     "grade_id": "out_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "`+-------+-----+\n",
    "|palabra|count|\n",
    "+-------+-----+\n",
    "|   Data|    2|\n",
    "|    UOC|    2|\n",
    "|    Big|    2|\n",
    "|  Spark|    1|\n",
    "+-------+-----+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51a6f675e66fec697c4bcc8539f04e5d",
     "grade": false,
     "grade_id": "cell-43d7b0107f924383",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A partir de este ejemplo que hemos visto y que el alumno debe ejecutar para probar su funcionamiento, se pide:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "415d9d6637dfc10900afa9801f260f7f",
     "grade": false,
     "grade_id": "task_1_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Pregunta 1. (1 punto)** Realiza un programa en Python que cuente las palabras que empiezan por A y que tengan más de 5 counts. Debéis ejecutarlo el programa en una **terminal**, no dentro del Jupyter, y en otra terminal el netcat. Para ello utilizamos un programa Python en local mediante `./python3 PEC5_2_1_1.py localhost <puerto_asignado>`  \n",
    "\n",
    "> Adjunta el código y la salida obtenida en **forma textual**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b83f758c0a481d9c888ee65ec2f2e7a2",
     "grade": false,
     "grade_id": "out_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "`+-------+-----+\n",
    "|palabra|count|\n",
    "+-------+-----+\n",
    "| Albert|    6|\n",
    "+-------+-----+`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24ec0f240694795c3124870a0bc9b857",
     "grade": true,
     "grade_id": "answer_1_1_a",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import sys\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "# Introducid el nombre de la app PEC5_ seguido de vuestro nombre de usuario\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_fmoralesh\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Creamos el DataFrame representando el streaming de las lineas que nos entran por host:port\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', sys.argv[1])\\\n",
    "    .option('port', sys.argv[2])\\\n",
    "    .load()\n",
    "\n",
    "# Separamos las lineas en palabras en un nuevo DF\n",
    "#las funciones explode y split estan explicadas en\n",
    "#https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html\n",
    "wordsDF = linesDF.select(\n",
    "    explode(\n",
    "        split(linesDF.value, ' ')\n",
    "    ).alias('palabra')\n",
    ")\n",
    "\n",
    "# Generamos el word count en tiempo de ejecución\n",
    "wordCountsDF = wordsDF.groupBy('palabra').count()\n",
    "\n",
    "# Iniciamos la consuta que muestra por consola o almacena en memoria el word count. \n",
    "# Trabajamos a partir del DataFrame que contiene la agrupación de las palabras y el numero de repeticiones\n",
    "# Utilizamos el formato memory para poder mostrarlo en Notebook, \n",
    "#si ejecutamos en consola debemos poner el formato console\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .start()\n",
    "\n",
    "#en una ejecución desde el terminal de sistema, necesitamos evitar que el programa finalice mientras \n",
    "#se está ejecutando la consulta en un Thread separado y en segundo plano. \n",
    "#query.awaitTermination() \n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT * FROM palabras WHERE palabra LIKE \"A%\" AND count > 5').show())\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71e62a24732355a28f1035d3968eebc1",
     "grade": false,
     "grade_id": "cell-d4ecb863c0062b6a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b61bd1ca809dfb57abf18170d66d1b49",
     "grade": true,
     "grade_id": "answer_1_1_b",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<code>\n",
    "+-------+-----+\n",
    "|palabra|count|\n",
    "+-------+-----+\n",
    "|      A|   15|\n",
    "|    Arg|    7|\n",
    "+-------+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a8f72ff828d9c6d0e06f999aee33a21",
     "grade": false,
     "grade_id": "task_1_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora vamos a realizar un ejercicio que nos permita realizar una consulta SQL sobre los datos recibidos.  Además, utilizaremos el mecanismo de control de fallos que Spark utiliza, los [*checkpoint*](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing), que van guardando información en el HDFS por si es necesario recuperarla. \n",
    "\n",
    ">**Pregunta 2. (1 punto)** Crea una tabla temporal para poder realizar una consulta SQL sobre las palabras que estamos obteniendo mediante streaming. El programa debe extraer las diferentes palabras de una frase y solo mostrar por consola aquellas que tengan una longitud superior a 3 caracteres. Tenéis que mostrar el tiempo de adquisición y poner un checkpoint en HDFS que se denomine `punto_control_pec5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2630185e0d18cabdc8ce0a38024304f6",
     "grade": false,
     "grade_id": "out_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "`\n",
    "+-------+--------------------+\n",
    "|palabra|              tiempo|\n",
    "+-------+--------------------+\n",
    "|   Data|2021-12-2  12:21:...|\n",
    "+-------+--------------------+\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e7cef53060e9d76daf780a37828d8b",
     "grade": true,
     "grade_id": "answer_1_2_a",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, length\n",
    "from pyspark.sql.functions import split, size, col\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "\n",
    "# Introducid el nombre de la app PEC5_ seguido de vuestro nombre de usuario\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_fmoralesh\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Creamos el DataFrame representando el streaming de las lineas que nos entran por host:port\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', sys.argv[1])\\\n",
    "    .option('port', sys.argv[2])\\\n",
    "    .option('includeTimestamp', 'true')\\\n",
    "    .load()\n",
    "\n",
    "# Separamos las lineas en palabras en un nuevo DF\n",
    "#las funciones explode y split estan explicadas en\n",
    "#https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html\n",
    "wordsDF = linesDF.select(\n",
    "    explode(\n",
    "        split(linesDF.value, ' ')\n",
    "    ).alias('palabra'),linesDF.timestamp\n",
    ")\n",
    "\n",
    "# Generamos el word count en tiempo de ejecución\n",
    "wordCountsDF = wordsDF.where(length(col(\"palabra\")) > 2)\\\n",
    "    .groupBy('timestamp','palabra').count()\n",
    "\n",
    "# Iniciamos la consuta que muestra por consola o almacena en memoria el word count. \n",
    "# Trabajamos a partir del DataFrame que contiene la agrupación de las palabras y el numero de repeticiones\n",
    "# Utilizamos el formato memory para poder mostrarlo en Notebook, \n",
    "#si ejecutamos en consola debemos poner el formato console\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .option(\"checkpointLocation\", \"punto_control_pec5\")\\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .start()\n",
    "\n",
    "#en una ejecución desde el terminal de sistema, necesitamos evitar que el programa finalice mientras \n",
    "#se está ejecutando la consulta en un Thread separado y en segundo plano. \n",
    "#query.awaitTermination() \n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT palabra, timestamp as tiempo FROM palabras').show())\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e260e5fc8f578fd0640158bb713f34fa",
     "grade": false,
     "grade_id": "cell-874ffab8d3c2535e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d850a84b41af627e43f354b5974fcad",
     "grade": true,
     "grade_id": "answer_1_2_b",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<code>\n",
    "+---------+--------------------+\n",
    "|  palabra|              tiempo|\n",
    "+---------+--------------------+\n",
    "|Mardision|2021-12-29 10:55:...|\n",
    "|   prueba|2021-12-29 10:55:...|\n",
    "|      una|2021-12-29 10:55:...|\n",
    "|     Esto|2021-12-29 10:55:...|\n",
    "+---------+--------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e751fd7c4fdba163f88d6f489ac78457",
     "grade": false,
     "grade_id": "task_1_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 3. (1 punto)** Modifica el programa para que haga uso del outputMode *append*. Debemos de guardar cada entrada en un fichero de texto en HDFS. Adjunta la salida del HDFS del contenido del directorio creado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71a3c0050f7856c05977c0b34dd6ba8b",
     "grade": false,
     "grade_id": "out_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "`\n",
    "hdfs dfs -ls /user/<usuario>/pec5_1_3\n",
    "Found 4 items\n",
    "drwxr-xr-x   - usuario usuario          0 2021-12-02 12:43 /user/<usuario>/pec5_1_3/_spark_metadata\n",
    "-rw-r--r--   3 usuario usuario          9 2021-12-02 12:43 /user/<usuario>/pec5_1_3/part-00000-499014ff-cf00-4f2f-a8a4-d282cdac1a19-c000.txt\n",
    "-rw-r--r--   3 usuario usuario          7 2021-12-02 12:43 /user/<usuario>/pec5_1_3/part-00000-7d8cb984-95ad-4e50-8887-a72f4d2814a2-c000.txt\n",
    "-rw-r--r--   3 usuario usuario          0 2021-12-02 12:43 /user/<usuario>/pec5_1_3/part-00000-a4918ce0-c930-439f-a5cd-a1bd777f609b-c000.txt\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "949ba4e58a1502950b1a315768eff26a",
     "grade": true,
     "grade_id": "answer_1_3_a",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, length\n",
    "from pyspark.sql.functions import split, size, col, concat, lit\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "\n",
    "# Introducid el nombre de la app PEC5_ seguido de vuestro nombre de usuario\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_fmoralesh\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Creamos el DataFrame representando el streaming de las lineas que nos entran por host:port\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', sys.argv[1])\\\n",
    "    .option('port', sys.argv[2])\\\n",
    "    .option('includeTimestamp', 'true')\\\n",
    "    .load()\n",
    "\n",
    "# Separamos las lineas en palabras en un nuevo DF\n",
    "#las funciones explode y split estan explicadas en\n",
    "#https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html\n",
    "wordsDF = linesDF.select(\n",
    "    explode(\n",
    "        split(linesDF.value, ' ')\n",
    "    ).alias('palabra'),linesDF.timestamp\n",
    ").where(length(col(\"palabra\")) > 2).select(concat(col(\"palabra\"), lit(\",\"), col(\"timestamp\")))\n",
    "\n",
    "# Generamos el word count en tiempo de ejecución\n",
    "wordCountsDF = wordsDF\n",
    "\n",
    "# Iniciamos la consuta que muestra por consola o almacena en memoria el word count. \n",
    "# Trabajamos a partir del DataFrame que contiene la agrupación de las palabras y el numero de repeticiones\n",
    "# Utilizamos el formato memory para poder mostrarlo en Notebook, \n",
    "#si ejecutamos en consola debemos poner el formato console\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .format(\"text\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"path\", \"/user/fmoralesh/pec5_1_3\")\\\n",
    "    .option(\"checkpointLocation\", \"./punto_control_pec5\")\\\n",
    "    .start()\n",
    "\n",
    "#en una ejecución desde el terminal de sistema, necesitamos evitar que el programa finalice mientras \n",
    "#se está ejecutando la consulta en un Thread separado y en segundo plano. \n",
    "query.awaitTermination() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17e76b1ae84cb6b3930194d0d1428016",
     "grade": false,
     "grade_id": "cell-4ea5b9edeae46258",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en el HDFS en formato de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68b2c35d48a8baeec7439a1c135616c9",
     "grade": true,
     "grade_id": "answer_1_3_b",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<code>\n",
    "hdfs dfs -ls /user/fmoralesh/pec5_1_3\n",
    "Found 16 items\n",
    "drwxr-xr-x   - fmoralesh fmoralesh          0 2021-12-28 17:06 /user/fmoralesh/pec5_1_3/ \n",
    "drwxr-xr-x   - fmoralesh fmoralesh          0 2021-12-28 17:06 /user/fmoralesh/pec5_1_3/%20\n",
    "drwxr-xr-x   - fmoralesh fmoralesh          0 2021-12-28 17:15 /user/fmoralesh/pec5_1_3/_spark_metadata\n",
    "-rw-r--r--   3 fmoralesh fmoralesh         36 2021-12-28 17:15 /user/fmoralesh/pec5_1_3/part-00000-046de910-8e83-4de9-b016-2b1c8723b31a-c000.txt\n",
    "-rw-r--r--   3 fmoralesh fmoralesh          0 2021-12-28 17:13 /user/fmoralesh/pec5_1_3/part-00000-0474cf19-fd0b-4336-acf2-2e441660a5c1-c000.txt\n",
    "-rw-r--r--   3 fmoralesh fmoralesh          0 2021-12-27 21:19 /user/fmoralesh/pec5_1_3/part-00000-5bb03b86-ab32-403c-98ff-b1c688d4d522-c000.txt\n",
    "-rw-r--r--   3 fmoralesh fmoralesh          0 2021-12-27 21:09 /user/fmoralesh/pec5_1_3/part-00000-68240808-9c06-423c-b5d7-7ad29b8dde53-c000.txt\n",
    "-rw-r--r--   3 fmoralesh fmoralesh         40 2021-12-28 17:15 /user/fmoralesh/pec5_1_3/part-00000-77545e69-5e90-4086-9bd8-40f85cc680aa-c000.txt\n",
    "-rw-r--r--   3 fmoralesh fmoralesh         33 2021-12-28 17:15 /user/fmoralesh/pec5_1_3/part-00000-7c6ec129-b107-42d1-bbd5-76f85aa8aeda-c000.txt\n",
    "-rw-r--r--   3 fmoralesh fmoralesh          0 2021-12-26 18:12 /user/fmoralesh/pec5_1_3/part-00000-7c7e5dd6-0b5f-473d-9fce-ddfaaeb27ae4-c000.txt\n",
    "-rw-r--r--   3 fmoralesh fmoralesh          0 2021-12-28 17:16 /user/fmoralesh/pec5_1_3/part-00000-bba0563c-cfbe-4971-86d2-47b151488a1c-c000.txt\n",
    "-rw-r--r--   3 fmoralesh fmoralesh          0 2021-12-27 21:17 /user/fmoralesh/pec5_1_3/part-00000-cb010a5b-f456-4ce3-9495-0f07addfd57e-c000.txt\n",
    "-rw-r--r--   3 fmoralesh fmoralesh          0 2021-12-28 17:15 /user/fmoralesh/pec5_1_3/part-00000-eaac067f-64b1-409c-ad18-32caab934cf3-c000.txt\n",
    "-rw-r--r--   3 fmoralesh fmoralesh         31 2021-12-28 17:15 /user/fmoralesh/pec5_1_3/part-00000-edc037f9-0430-40e6-abe7-bbc0ccff3541-c000.txt\n",
    "-rw-r--r--   3 fmoralesh fmoralesh          0 2021-12-27 21:18 /user/fmoralesh/pec5_1_3/part-00000-f1df973d-e37a-4727-a8ff-38e13b3c0569-c000.txt\n",
    "-rw-r--r--   3 fmoralesh fmoralesh         88 2021-12-27 21:17 /user/fmoralesh/pec5_1_3/part-00000-fcbbe6f6-814c-45f1-9aab-8f599a59c6c8-c000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cbde4a8ca1e01e2a478b4bff3b1ff74",
     "grade": false,
     "grade_id": "task_1_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 4.(1 punto)** Realiza un programa en Python para que haga uso del outputMode update, y que los datos entrantes por consola. La lectura debe realizar en intervalos de 5 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d6597dd5ca80757219b1f530ad883df",
     "grade": false,
     "grade_id": "out_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "-------------------------------------------\n",
    "Batch: 5\n",
    "-------------------------------------------\n",
    "+-------+\n",
    "|palabra|\n",
    "+-------+\n",
    "|    Big|\n",
    "|   Data|\n",
    "| Hadoop|\n",
    "+-------+\n",
    "<5 ... segundos>\n",
    "-------------------------------------------\n",
    "Batch: 6\n",
    "-------------------------------------------\n",
    "+-------+\n",
    "|palabra|\n",
    "+-------+\n",
    "|  Spark|\n",
    "+-------+\n",
    "    </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6aebc7fbc4c5063ca03a82cf3eb8754",
     "grade": true,
     "grade_id": "answer_1_4_a",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, length\n",
    "from pyspark.sql.functions import split, size, col\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "\n",
    "# Introducid el nombre de la app PEC5_ seguido de vuestro nombre de usuario\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_fmoralesh\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Creamos el DataFrame representando el streaming de las lineas que nos entran por host:port\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', sys.argv[1])\\\n",
    "    .option('port', sys.argv[2])\\\n",
    "    .load()\n",
    "\n",
    "# Separamos las lineas en palabras en un nuevo DF\n",
    "#las funciones explode y split estan explicadas en\n",
    "#https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html\n",
    "wordsDF = linesDF.select(\n",
    "    explode(\n",
    "        split(linesDF.value, ' ')\n",
    "    ).alias('palabra'))\n",
    "\n",
    "# Generamos el word count en tiempo de ejecución\n",
    "wordCountsDF = wordsDF\n",
    "\n",
    "# Iniciamos la consuta que muestra por consola o almacena en memoria el word count. \n",
    "# Trabajamos a partir del DataFrame que contiene la agrupación de las palabras y el numero de repeticiones\n",
    "# Utilizamos el formato memory para poder mostrarlo en Notebook, \n",
    "#si ejecutamos en consola debemos poner el formato console\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .option(\"path\", \"./user/fmoralesh/pec5_1_4\")\\\n",
    "    .option(\"checkpointLocation\", \"./punto_control_pec5\")\\\n",
    "    .trigger(processingTime=\"5 second\")\\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "#en una ejecución desde el terminal de sistema, necesitamos evitar que el programa finalice mientras \n",
    "#se está ejecutando la consulta en un Thread separado y en segundo plano. \n",
    "#query.awaitTermination() \n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a151c7925f831e68b21582124d49707",
     "grade": false,
     "grade_id": "cell-f981a227281aa19e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d61e4bbff433e54388bf72817ddb9ff3",
     "grade": true,
     "grade_id": "answer_1_4_b",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<code>\n",
    "-------------------------------------------\n",
    "Batch: 7\n",
    "-------------------------------------------\n",
    "+-------+\n",
    "|palabra|\n",
    "+-------+\n",
    "|   ESto|\n",
    "|     es|\n",
    "|    una|\n",
    "| prueba|\n",
    "+-------+\n",
    "-------------------------------------------\n",
    "Batch: 8\n",
    "-------------------------------------------\n",
    "+-------+\n",
    "|palabra|\n",
    "+-------+\n",
    "|    Eje|\n",
    "|      4|\n",
    "+-------+\n",
    "    </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45eb40cf552c9e4da2678d4fd1e2815a",
     "grade": false,
     "grade_id": "task_1_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 5.(1 punto)** Explica las diferencias y similitudes entre los tipos de salidas existentes en Structured Streaming (complete, update y append). El texto debe ser claro, explicativo y tener una extensión de 10 líneas aproximadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee8eef197e3fb242072795189cebaeff",
     "grade": true,
     "grade_id": "answer_1_5",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "* Complete. Outputmode con todos las filas del dataset que tengamos en el flujo y que queramos volcar en el sink cada vez que haya una actualización. Utilizaremos este modo cuando queramos ir añadiendo información agregada al sink cada vez que se active un trigger. En este modo sólo se usa información agregada con alguna función tipo count, sum, etc.\n",
    "* Append. Output mode con únicamente las nuevas filas que vayamos añadiendo en el flujo y que volcaremos en el sink.\n",
    "* Update. Output similar al anterior, con la diferencia que volcaremos en el Sink únicamente la nuevas filas comparando lo que teníamos en el estado anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "784f0c45d66c91f2eda7de23db4814e2",
     "grade": false,
     "grade_id": "intro_2_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## PARTE 2. Operaciones de ventana sobre eventos temporales\n",
    "En esta parte vamos a trabajar con operaciones de ventana sobre eventos temporales. Para ello vamos a utilizar el formato *rate*. El source [RateStreamSource](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes) es una fuente de transmisión que genera números consecutivos con marca de tiempo y es utilizada habitualmente para hacer pruebas y PoC (*Proof of Concept*). Para configurar un RateStreamSource utilizaremos  `format('rate')`, y el esquema de los datos entrantes es el adjunto siguiente. A diferencia de los ejercicios de la parte 1 no tendremos dos programas corriendo simultáneamente en el terminal, solo tendremos una, la de nuestro programa pyspark, dado que el formato rate se controla directamente desde la configuración del source Spark.\n",
    "\n",
    "\n",
    "`root\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- value: long (nullable = true) `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "feb4bcc33ff3a306bc9f4614eb2e493f",
     "grade": false,
     "grade_id": "task_2_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 1. (1 punto)** Realiza un programa mediante Structured Streaming que genere números mediante un formato *rate* como origen del streaming y donde debéis realizar el tratamiento de los mismos para acumularlos. Los números deben generarse cada segundo, y debemos utilizar una ventana de agrupación del streaming de 10 segundos y que se actualice cada 5 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85721b3e66a790221f30a43543326db4",
     "grade": true,
     "grade_id": "answer_2_1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, length\n",
    "from pyspark.sql.functions import split, size, col, window\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_fmoralesh\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('rate').option(\"rowsPerSecond\", 1).load()\n",
    "\n",
    "# Separamos las lineas en palabras en un nuevo DF\n",
    "#las funciones explode y split estan explicadas en\n",
    "#https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html\n",
    "wordsDF = linesDF\n",
    "\n",
    "# Generamos el word count en tiempo de ejecución\n",
    "\n",
    "wordCountsDF = wordsDF.withColumn(\n",
    "    \"window\",\n",
    "    window(\n",
    "         \"timestamp\", \n",
    "         windowDuration=\"10 seconds\"\n",
    "    )\n",
    ").groupBy(\"window\", \"value\").count().select(\"window\", \"value\", \"count\")\n",
    "\n",
    "\n",
    "# Iniciamos la consuta que muestra por consola o almacena en memoria el word count. \n",
    "# Trabajamos a partir del DataFrame que contiene la agrupación de las palabras y el numero de repeticiones\n",
    "# Utilizamos el formato memory para poder mostrarlo en Notebook, \n",
    "#si ejecutamos en consola debemos poner el formato console\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('update')\\\n",
    "    .format(\"console\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .trigger(processingTime=\"5 second\")\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "#while True:\n",
    "#    clear_output(wait=True)\n",
    "#    display(query.status)\n",
    "#    sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0bd3551f6a2a2734a224516689efaf6",
     "grade": false,
     "grade_id": "task_2_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 2. (1 punto)** Comenta el código, muestra la salida que has obtenido y coméntala en una extensión en 4 y 8 líneas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75c6bca1f50a1eaa52db1b45a797699e",
     "grade": false,
     "grade_id": "out_7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "-------------------------------------------                                     \n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+------------------------------------------+-----+-----+\n",
    "|window                                    |value|count|\n",
    "+------------------------------------------+-----+-----+\n",
    "|[2021-12-03 10:33:40, 2021-12-03 10:33:50]|0    |1    |\n",
    "|[2021-12-03 10:33:45, 2021-12-03 10:33:55]|5    |1    |\n",
    "|[2021-12-03 10:33:45, 2021-12-03 10:33:55]|0    |1    |\n",
    "|[2021-12-03 10:33:45, 2021-12-03 10:33:55]|3    |1    |\n",
    "|[2021-12-03 10:33:45, 2021-12-03 10:33:55]|4    |1    |\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cecd0636f84a9495da34157226c550d",
     "grade": true,
     "grade_id": "answer_2_2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<code>\n",
    "-------------------------------------------                                     \n",
    "Batch: 5\n",
    "-------------------------------------------\n",
    "+------------------------------------------+-----+-----+\n",
    "|window                                    |value|count|\n",
    "+------------------------------------------+-----+-----+\n",
    "|[2021-12-29 12:52:10, 2021-12-29 12:52:20]|26   |1    |\n",
    "|[2021-12-29 12:52:20, 2021-12-29 12:52:30]|28   |1    |\n",
    "|[2021-12-29 12:52:20, 2021-12-29 12:52:30]|30   |1    |\n",
    "|[2021-12-29 12:52:20, 2021-12-29 12:52:30]|29   |1    |\n",
    "|[2021-12-29 12:52:20, 2021-12-29 12:52:30]|27   |1    |\n",
    "+------------------------------------------+-----+-----+   \n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código es muy similar al los anterior con las siguientes diferencias:\n",
    "* El Spark lo creamos con el formato \"rate\" y lo configuramos para que genere una línea por segundo.\n",
    "* Agrupamos el dataframe creando una ventana con un intervalo de 10 segundos.\n",
    "* Lo sacamos en pantalla con el modo update cy configuramos el Trigger para que se lance cada 5 segundos.\n",
    "\n",
    "El resultado es que vamos generando una serie de valores que vamos guardando junto al timestamp de creación y las veces que se ha repetido en ese Batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3f816157b1003d0b6cff5e88c3dfd7a",
     "grade": false,
     "grade_id": "task_2_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En la ejecución de las consultas es muy interesante poder ir obteniendo información sobre el progreso realizado en el último disparador del flujo: qué datos se procesaron, cuáles fueron las tasas de procesamiento, latencias, etc.\n",
    "\n",
    ">**Pregunta 3 (1 punto).** Modifica el programa para que muestre 3 [métricas](https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#monitoring-streaming-queries) del streaming mientras este se realiza. Solo se deben mostrar métricas mientras la consulta está activa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b76c64b787e16d2d540a1383742c8a8",
     "grade": true,
     "grade_id": "answer_2_3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "import sys\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import col, window\n",
    "from time import sleep\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Introducid el nombre de la app PEC5_ seguido de vuestro nombre de usuario\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_fmoralesh\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Creamos el DataFrame representando el streaming de las lineas que nos entran por host:port\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('rate').option(\"rowsPerSecond\", 1).load()\n",
    "\n",
    "# Separamos las lineas en palabras en un nuevo DF\n",
    "#las funciones explode y split estan explicadas en\n",
    "#https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html\n",
    "wordsDF = linesDF\n",
    "\n",
    "# Generamos el word count en tiempo de ejecución\n",
    "\n",
    "wordCountsDF = wordsDF.withColumn(\n",
    "    \"window\",\n",
    "    window(\n",
    "         \"timestamp\", \n",
    "         windowDuration=\"10 seconds\"\n",
    "    )\n",
    ").groupBy(\"window\", \"value\").count().select(\"window\", \"value\", \"count\")\n",
    "\n",
    "\n",
    "# Iniciamos la consuta que muestra por consola o almacena en memoria el word count. \n",
    "# Trabajamos a partir del DataFrame que contiene la agrupación de las palabras y el numero de repeticiones\n",
    "# Utilizamos el formato memory para poder mostrarlo en Notebook, \n",
    "#si ejecutamos en consola debemos poner el formato console\n",
    "\n",
    "\n",
    "query = wordCountsDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('update')\\\n",
    "    .format(\"console\") \\\n",
    "    .queryName(\"palabras\") \\\n",
    "    .trigger(processingTime=\"5 second\")\\\n",
    "    .start()\n",
    "\n",
    "query_progress =  query.lastProgress\n",
    "print(\"progress \", query_progress)\n",
    "print(\"status \", query.status)\n",
    "print(\"active \", query.isActive)\n",
    "\n",
    "spark.conf.set(\"spark.sql.streaming.metricsEnabled\", \"true\")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57da1fc6fca61d2cec1dcb840787e03a",
     "grade": false,
     "grade_id": "task_2_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 4 (1 punto).** Explica las 3 métricas aplicadas con una extensión entre 5 y 10 líneas propias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff223ee18dd86791e02181cbb66402fc",
     "grade": false,
     "grade_id": "out_8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "{'isDataAvailable': False, 'isTriggerActive': False, 'message': 'Initializing sources'}\n",
    "{'stateOperators': [{'customMetrics': {'loadedMapCacheHitCount': 0, 'stateOnCurrentVersionSizeBytes': 25198, 'loadedMapCacheMissCount': 0}, 'numRowsUpdated': 0, 'memoryUsedBytes': 82798, 'numRowsTotal': 0}], 'timestamp': '2021-12-03T10:16:01.657Z', 'sources': [{'description': 'RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default', 'endOffset': 0, 'startOffset': None, 'processedRowsPerSecond': 0.0, 'numInputRows': 0}], 'runId': '9134994e-c17f-4277-974a-21cd09cf9aea', 'durationMs': {'triggerExecution': 36450, 'walCommit': 48, 'getB[Stage 8:======>        (22 + 2) / 200]\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68677f26c2ac1f5b13b4695f87a6fbdb",
     "grade": true,
     "grade_id": "answer_2_4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<code>\n",
    "{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
    "{'batchId': 7, 'stateOperators': [{'numRowsUpdated': 5, 'memoryUsedBytes': 89183, 'numRowsTotal': 39, 'customMetrics': {'loadedMapCacheHitCount': 2800, 'loadedMapCacheMissCount': 0, 'stateOnCurrentVersionSizeBytes': 25615}}], 'durationMs': {'addBatch': 1938, 'queryPlanning': 42, 'walCommit': 67, 'setOffsetRange': 0, 'getEndOffset': 0, 'triggerExecution': 2107, 'getBatch': 0}, 'timestamp': '2021-12-29T14:53:45.000Z', 'id': '091a4fb1-6700-4b23-a1c4-6d1c4a90aef2', 'name': 'palabras', 'numInputRows': 5, 'processedRowsPerSecond': 2.3730422401518743, 'runId': 'e5f13762-27ae-4265-9878-90901ca816b4', 'inputRowsPerSecond': 1.0, 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@46db697b'}, 'sources': [{'startOffset': 34, 'description': 'RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default', 'numInputRows': 5, 'processedRowsPerSecond': 2.3730422401518743, 'inputRowsPerSecond': 1.0, 'endOffset': 39}]}\n",
    "</code>\n",
    "\n",
    "Las métricas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ca4dccb18ba42b31ab1988999714669",
     "grade": false,
     "grade_id": "intro_3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## PARTE 3. Captura y procesamiento de datos en tiempo real con la API OpenSky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6821ac85e43507dabf5a8ff513cf8822",
     "grade": false,
     "grade_id": "intro_3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Es esta parte de la práctica vamos a trabajar la adquisición de datos en tiempo real de [OpenSky](https://opensky-network.org/). OpenSky Network es una asociación sin ánimo de lucro con sede en Suiza que brinda acceso abierto a los datos de control de seguimiento de vuelos.  Fue creado como un proyecto de investigación por varias universidades y entidades gubernamentales con el objetivo de mejorar la seguridad, confiabilidad y eficiencia del espacio aéreo. Su función principal es recopilar, procesar y almacenar datos de control de tráfico aéreo y proporcionar acceso abierto a estos datos al público. Esencialmente los datos de los aviones se obtienen vía satélite haciendo uso de  Automatic Dependent Surveillance–Broadcast (ADS–B). Para realizar este ejercicio no es necesario registrarse en el sistema OpenSky dado que vamos ha relizar actualizaciones de la información e vuelo sobre la superficie de España cada 10 segundos. La API está disponible este [enlace](https://openskynetwork.github.io/opensky-api/python.html). El parámetro bbox es una tupla que indica la latitud mínima, máxima, y las longitudes mínimas y máximas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f204eac8ff253369dd44eb65beafd35",
     "grade": false,
     "grade_id": "intro_3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Primeramente, vamos a utilizar el servicio OpenSkyApi para leer un rectángulo con las latitudes y longitudes que engloban la península ibérica.\n",
    "\n",
    "Para ello debéis [instalar](https://github.com/openskynetwork/opensky-api) la biblioteca en vuestro directorio del servidor Cloudera\n",
    "\n",
    "1. Descargar en formato .zip el repositorio\n",
    "1. Subir a vuestro directorio personal del servidor de Cloudera el zip. \n",
    "1. Descomprimirlo.\n",
    "1. Dentro del directorio que ha creador ejecutar `pip install -e ./python`\n",
    "\n",
    "Una vez instalada el módulo anterior, la siguiente celda os mostrará los vuelos registrados sobre la península ibérica en estos momentos. Observad con detenimiento las propiedades del diccionario de cada vuelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba1422e695cfac097b08aaabe243cdcf",
     "grade": false,
     "grade_id": "into_3_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from random import sample\n",
    "\n",
    "from opensky_api import OpenSkyApi\n",
    "api = OpenSkyApi()\n",
    "states = api.get_states(bbox=(36.173357, 44.024422,-10.137019, 1.736138))\n",
    "#recuperamos codigo, pais_origen, long, lat, altitud, velocidad, ratio_vertical\n",
    "#atención en este ejemplo solo estamos mostrando 5 vuelos aleatorios, \n",
    "#en vuestros ejercicios deberéis eliminar la función sample\n",
    "for s in sample(states.states,5):\n",
    "    vuelo_dict = {\n",
    "                'callsign':s.callsign,\n",
    "                'country': s.origin_country,\n",
    "                'longitude': s.longitude,\n",
    "                'latitude': s.latitude,\n",
    "                'velocity': s.velocity,\n",
    "                'vertical_rate': s.vertical_rate,\n",
    "            }\n",
    "    vuelo_encode_data = json.dumps(vuelo_dict, indent=2).encode('utf-8')\n",
    "    print(\"(%r, %r,%r, %r, %r, %r)\" % (s.callsign, s.origin_country, s.longitude, s.latitude,s.velocity,s.vertical_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4cf6b46d367d23ac0646e61dc4437e4",
     "grade": false,
     "grade_id": "out_10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "`('BAW457  ', 'United Kingdom',-3.5196, 40.4292, 86.45, 10.73)\n",
    "('BLX245  ', 'Sweden',-6.0307, 43.8266, 252.51, 0)\n",
    "('CFG1HE  ', 'Germany',-8.4689, 40.2967, 236.56, 0)\n",
    "('TOM3MK  ', 'United Kingdom',-7.2687, 41.5878, 247.02, 0)\n",
    "('AEA57MC ', 'Spain',-0.5364, 38.2791, 64.7, -3.9)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30a4aef1194952c5cccc756fffa5d04b",
     "grade": false,
     "grade_id": "into_3_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora vamos a crear un programa en Python para poder enviar cada 10 segundos por el puerto que tenéis asignado información de los vuelos que hay sobre la península ibérica. Deberéis poner en marcha primero el programa Python con el servidor de sockets que lee de Opensky y después el programa de Spark con structured streaming, es decir, en esta parte volvemos a tener dos terminales abiertas a la vez, y lo podéis realizar con el VSCode o con el SSH."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5df565ecbd6b9daf81cb3d2934a363a",
     "grade": false,
     "grade_id": "task_3_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 1. (1 punto)** Modifica el programa Python para enviar datos de los vuelos en formato JSON. Os podéis auxiliar de la función [json.dumps](https://docs.python.org/3/library/json.html) que nos permite crear un JSON binario de cada diccionario con las propiedades del vuelo. Prestad atención al salto de línea, '\\n', que se adjunta al final de cada envío, es fundamental para cerrar la transmisión de datos a Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d2da5d413e7fd00bc872d823c08180f",
     "grade": true,
     "grade_id": "answer_3_1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import socket\n",
    "import json\n",
    "from opensky_api import OpenSkyApi\n",
    "\n",
    "HOST = 'localhost'  # hostname o IP address\n",
    "PORT = 20036        # puerto socket server\n",
    "\n",
    "api = OpenSkyApi()\n",
    "states = api.get_states(bbox=(36.173357, 44.024422,-10.137019, 1.736138))\n",
    "\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.bind((HOST, PORT))\n",
    "s.listen(1)\n",
    "while True:\n",
    "    print('\\Esperando un cliente a',HOST , PORT)\n",
    "    conn, addr = s.accept()\n",
    "    print('\\Connectat per', addr)\n",
    "    try:\n",
    "        while(True):\n",
    "            v = {}\n",
    "            for vuelo in states.states:\n",
    "                vuelo_dict = {\n",
    "                'callsign':vuelo.callsign,\n",
    "                'country': vuelo.origin_country,\n",
    "                'longitude': vuelo.longitude,\n",
    "                'latitude': vuelo.latitude,\n",
    "                'velocity': vuelo.velocity,\n",
    "                'vertical_rate': vuelo.vertical_rate,\n",
    "                }\n",
    "                v = json.dumps(vuelo_dict, indent=2).encode('utf-8')\n",
    "                print(v)\n",
    "                conn.send(v)\n",
    "                conn.send(b'\\n')\n",
    "            sleep(10)       \n",
    "    except socket.error:\n",
    "        print ('Error .\\n\\nClient desconnectat.\\n')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca28e20d52c477f90cd98b43a87b31a9",
     "grade": false,
     "grade_id": "task_3_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 2. (1 punto)** Se pide leer los datos recibidos mediante structured streaming y mostrar el esquema de los datos recibidos. En este primer ejercicio solo vamos a tener una cadena con el JSON recibido de cada vuelo y un esquema con un único elemento. Debéis utilizar la función \"printSchema()\".\n",
    "\n",
    ">Una vez comprobada que la transmisión funciona, se pide realizar un pre-procesado antes del envío de los datos mediante el socket para eliminar aquellas líneas de datos que no sean útiles ni convenientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b20b6823016a47894ea3d4c91cd802a4",
     "grade": false,
     "grade_id": "out_11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "root\n",
    " |-- value: string (nullable = true)\n",
    "\n",
    "|value             |\n",
    "\n",
    "|{\"velocity\": 210.12, \"vertical_rate\": 0, \"latitude\": 43.6082, \"callsign\": \"TAP441  \", \"longitude\": -1.3992, \"country\": \"Portugal\"}         |\n",
    "|{\"velocity\": 246.5, \"vertical_rate\": 0, \"latitude\": 40.5836, \"callsign\": \"TAP844  \", \"longitude\": -3.8452, \"country\": \"Portugal\"}          |\n",
    "|{\"velocity\": 0, \"vertical_rate\": null, \"latitude\": 40.487, \"callsign\": \"IBE2800 \", \"longitude\": -3.5889, \"country\": \"Spain\"}      \n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03d6aa1be4af17718b153ab9f1670237",
     "grade": true,
     "grade_id": "answer_3_2_1",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import sys\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode,col\n",
    "from pyspark.sql.functions import split\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_fmoralesh\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', 'localhost')\\\n",
    "    .option('port', 20036)\\\n",
    "    .load()\n",
    "\n",
    "vuelosDF = linesDF.select(\n",
    "    explode(\n",
    "        split(linesDF.value, '\\n')\n",
    "    ).alias('vuelos')\n",
    ")\n",
    "\n",
    "vuelosDF = vuelosDF.filter(col(\"vuelos\").rlike(\"^(?!.*null.*).*$\"))\n",
    "\n",
    "query = vuelosDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('update')\\\n",
    "    .format(\"console\") \\\n",
    "    .queryName(\"vuelos\") \\\n",
    "    .option('truncate', 'false')\\\n",
    "    .start()\n",
    "\n",
    "\n",
    "while query.isActive:\n",
    "    print(\"\\n\")\n",
    "    vuelosDF.printSchema()\n",
    "    sleep(5)\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5456f5a970048c1ac355f71fa0b7d150",
     "grade": false,
     "grade_id": "cell-cf210fa5a9e69efd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcb6ebd68d329632f046ee871a8a44be",
     "grade": true,
     "grade_id": "answer_3_2_2",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<code>\n",
    "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
    "|vuelos                                                                                                                                 |\n",
    "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
    "|{\"latitude\": 40.6098, \"callsign\": \"TAP649M \", \"velocity\": 220.6, \"longitude\": -7.2705, \"vertical_rate\": -7.15, \"country\": \"Portugal\"}  |\n",
    "|{\"latitude\": 38.5234, \"callsign\": \"ANE80ZK \", \"velocity\": 138.52, \"longitude\": -2.0891, \"vertical_rate\": -0.33, \"country\": \"Spain\"}    |\n",
    "|{\"latitude\": 43.0884, \"callsign\": \"IBS36TB \", \"velocity\": 236.73, \"longitude\": -2.1074, \"vertical_rate\": 0, \"country\": \"Spain\"}        |\n",
    "|{\"latitude\": 39.43, \"callsign\": \"IBS3920 \", \"velocity\": 219.11, \"longitude\": 0.3919, \"vertical_rate\": 0, \"country\": \"Spain\"}           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ade4c0065f3d78a383ae78d02319a349",
     "grade": false,
     "grade_id": "task_3_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 3. (1 punto)** Se pide mostrar la información en forma de tabla, con las columnas, country|callsign|longitude|latitude|velocity|vertical_rate. Para ello vais a tener que crear un esquema mediante [StructType](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html) y aplicarlo a la función SQL [from_json](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.from_json.html). De esta manera podremos pasar de una columna string con todo el JSON, a 6 columnas con el tipo ajustado al valor contenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "375fbcfcd617d3b84c0ad94883628627",
     "grade": false,
     "grade_id": "out_12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "root\n",
    " |-- callsign: string (nullable = true)\n",
    " |-- country: string (nullable = true)\n",
    " |-- longitude: long (nullable = true)\n",
    " |-- latitude: long (nullable = true)\n",
    " |-- velocity: long (nullable = true)\n",
    " |-- vertical_rate: long (nullable = true)\n",
    "\n",
    "\n",
    "+--------------+--------+---------+--------+--------+-------------+\n",
    "|       country|callsign|longitude|latitude|velocity|vertical_rate|\n",
    "+--------------+--------+---------+--------+--------+-------------+\n",
    "|      Portugal|TAP441  |  -1.3992| 43.6082|  210.12|          0.0|\n",
    "|      Portugal|TAP844  |  -3.8452| 40.5836|   246.5|          0.0|\n",
    "|         Spain|IBE2800 |  -3.5889|  40.487|     0.0|         null|\n",
    "|United Kingdom|ABW713  |  -0.5287|  41.899|  155.82|        -7.48|\n",
    "|         Spain|FYS161  |   -0.188| 38.8394|   64.77|        -2.93|\n",
    "|         Spain|IBE3242 |  -3.5896| 40.4919|    0.77|         null|\n",
    "|         Spain|AEA4025 |   0.1208| 38.5346|  125.53|         3.25|\n",
    "|         Spain|P21     |  -3.5728| 40.4751|   10.29|         null|\n",
    "|         Spain|IBE30EA |  -2.6086|  40.914|  181.29|        -6.18|\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17662377ee9afa8dc6e11941824a72f6",
     "grade": true,
     "grade_id": "answer_3_3_1",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import sys\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import FloatType, StringType, StructField, StructType\n",
    "from pyspark.sql.functions import explode, from_json, col\n",
    "from pyspark.sql.functions import split\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PEC5_fmoralesh\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "linesDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', 'localhost')\\\n",
    "    .option('port', 20036)\\\n",
    "    .load()\n",
    "\n",
    "schema = StructType([\n",
    "            StructField('country', StringType(), True),\n",
    "            StructField('callsign', StringType(), True),\n",
    "            StructField('longitude', FloatType(), True),\n",
    "            StructField('latitude', FloatType(), True),\n",
    "            StructField('velocity', FloatType(), True),\n",
    "            StructField('vertical_rate', FloatType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "vuelosDF = linesDF.select(\n",
    "    explode(\n",
    "        split(linesDF.value, '\\n')\n",
    "    ).alias('vuelos')\n",
    ")\n",
    "\n",
    "vuelosDF = vuelosDF.withColumn(\"json\",from_json(col('vuelos'),schema)) \\\n",
    "                    .select('json.*')\\\n",
    "                    .filter(col(\"vuelos\").rlike(\"^(?!.*null.*).*$\"))\n",
    "\n",
    "query = vuelosDF\\\n",
    "    .writeStream\\\n",
    "    .outputMode('update')\\\n",
    "    .format(\"console\") \\\n",
    "    .queryName(\"vuelos\") \\\n",
    "    .option('truncate', 'false')\\\n",
    "    .start()\n",
    "\n",
    "\n",
    "while query.isActive:\n",
    "    print(\"\\n\")\n",
    "    vuelosDF.printSchema()\n",
    "    sleep(5)\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c06cd85ac27df93beefb9fe3f123c39",
     "grade": false,
     "grade_id": "cell-07353f460967c893",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "366df90301f27f080a894c1808edf125",
     "grade": true,
     "grade_id": "answer_3_3_2",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<code>\n",
    "root\n",
    " |-- country: string (nullable = true)\n",
    " |-- callsign: string (nullable = true)\n",
    " |-- longitude: float (nullable = true)\n",
    " |-- latitude: float (nullable = true)\n",
    " |-- velocity: float (nullable = true)\n",
    " |-- vertical_rate: float (nullable = true)\n",
    "\n",
    "\n",
    "+-----------+--------+---------+--------+--------+-------------+\n",
    "|country    |callsign|longitude|latitude|velocity|vertical_rate|\n",
    "+-----------+--------+---------+--------+--------+-------------+\n",
    "|Portugal   |TAP1357 |-8.2756  |40.4173 |211.39  |-4.88        |\n",
    "|Spain      |VOE3591 |1.2644   |40.6174 |215.66  |0.0          |\n",
    "|Austria    |EJU1602 |-0.8185  |39.0415 |225.74  |0.0          |\n",
    "|Spain      |IBS3905 |-7.5109  |36.8851 |219.92  |0.0          |\n",
    "|Hungary    |WZZ8284 |1.5146   |39.3735 |216.73  |-0.33        |\n",
    "|Spain      |VLG8829 |-1.4563  |43.0828 |236.3   |0.0          |\n",
    "|Spain      |IBE32BP |-3.0783  |40.8207 |196.76  |14.63        |\n",
    "|Austria    |EJU71RZ |-5.0604  |36.2494 |214.78  |0.0          |\n",
    "|Spain      |VLG8159 |0.3431   |41.8262 |233.26  |0.0          |\n",
    "|Sweden     |NSZ2CV  |-8.4239  |38.0333 |224.51  |0.0          |\n",
    "|Belgium    |OOMBP   |-3.5812  |38.5875 |147.37  |-7.48        |\n",
    "|Austria    |EJU71PD |-3.6842  |39.7705 |220.64  |0.0          |\n",
    "|France     |AFR28KX |-1.574   |42.9226 |225.93  |0.0          |\n",
    "|Spain      |IBE32GT |-2.0457  |42.2193 |239.73  |-4.88        |\n",
    "|Switzerland|EZS902K |0.4305   |43.131  |218.97  |0.0          |\n",
    "|Spain      |ANE8865 |-0.7192  |38.592  |196.81  |11.05        |\n",
    "|Spain      |AEA6096 |0.2407   |39.5875 |214.85  |0.0          |\n",
    "|Spain      |ANE15KE |-2.6365  |40.5944 |157.56  |-8.13        |\n",
    "|Sweden     |SAS6004 |-3.4237  |39.672  |222.55  |-0.33        |\n",
    "|Portugal   |TAP836A |-6.7827  |39.6987 |218.22  |0.0          |\n",
    "+-----------+--------+---------+--------+--------+-------------+\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f1cb2896ad74d3c09bfa59b17677916",
     "grade": false,
     "grade_id": "task_3_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 4. (1 punto)** Muestra el total de vuelos para cada destino agrupados por país de destino que hay en cada momento. Los datos deben mostrarse ordenados por país alfabéticamente. Tened en cuenta que podemos recibir datos duplicados dado que el script de OpenSky lee cada 10 segundos todos los vuelos existentes y los envía por socket. Por defecto Spark crea 200 tareas (cada una implicará una partición de los datos) por stage en el procesamiento en Structured Streaming. Para acelerar el proceso de captura, se pide que [ajustéis](https://spark.apache.org/docs/latest/sql-performance-tuning.html#other-configuration-options) el parámetro en la configuración de SparkSession a un valor de 4 particiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc257a5fbb2e9db8ca67f5490044aef8",
     "grade": false,
     "grade_id": "out_13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "-------------------------------------------\n",
    "Batch: 4\n",
    "-------------------------------------------\n",
    "+--------------------------+-----+\n",
    "|country                   |count|\n",
    "+--------------------------+-----+\n",
    "|Algeria                   |1    |\n",
    "|Austria                   |3    |\n",
    "|Belgium                   |1    |\n",
    "|Chile                     |2    |\n",
    "|Denmark                   |1    |\n",
    "|France                    |15   |\n",
    "|Germany                   |15   |\n",
    "|Hungary                   |1    |\n",
    "|Ireland                   |26   |\n",
    "|Kingdom of the Netherlands|2    |\n",
    "|Lithuania                 |1    |\n",
    "|Luxembourg                |1    |\n",
    "|Malta                     |5    |\n",
    "|Mexico                    |1    |\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fda41a55d99c7f0c38e05b804812cef",
     "grade": true,
     "grade_id": "answer_3_4_1",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95d206d775b46cf03921a4ad5ae7eb29",
     "grade": false,
     "grade_id": "cell-a2a12feeee643a28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b848a83be1b3c5d91e85976c88fc1d7c",
     "grade": true,
     "grade_id": "answer_3_4_1_2",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa7479591bce24fb7c0d521cd32b7325",
     "grade": false,
     "grade_id": "task_3_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 5. (1 punto)** Agrupa todos los vuelos que están subiendo en altura, los que están bajando y los que están en tierra. Indica su numero. Deberás auxiliarte de una consulta SQL para poder indicar con -1 que un vuelo está descendiendo, +1 si está subiendo, y 0 si está en tierra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65db97c981a4344ce7cf80027f6220d1",
     "grade": false,
     "grade_id": "out_14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Salida de ejemplo:\n",
    "\n",
    "<code>\n",
    "+------+-----+\n",
    "|estado|count|\n",
    "+------+-----+\n",
    "|     0|   96|\n",
    "|    -1|   54|\n",
    "|     1|   41|\n",
    "+------+-----+\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8aca0ff370a46419939efbdb1aa6c80",
     "grade": true,
     "grade_id": "answer_3_5_1",
     "locked": false,
     "points": 1.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49c7cbd77f16864bdcd875c08bf48f6e",
     "grade": false,
     "grade_id": "cell-a528e031ad020c15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copia la salida obtenida en formato de texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9fe7de196b10377bceb1a57d5245769",
     "grade": true,
     "grade_id": "answer_3_5_2",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d681d971225d4b5238e30c1968fd6bdf",
     "grade": false,
     "grade_id": "task_3_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 6. (1 punto)** ¿De que manera podemos identificar que aparece un nuevo vuelo en el espacio aéreo?. No hace falta escribir el código sino describir con palabras como se plantearía la solución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a0ab2e972f7fae3ffec91c08e983043",
     "grade": true,
     "grade_id": "answer_3_6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "352a02192f44fb4c5b21b0018f632fe5",
     "grade": false,
     "grade_id": "task_3_7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    ">**Pregunta 7. (1 punto)** Explica brevemente en una extensión de entre 5 y 10 lineas las ventajas e inconvenientes de utilizar Structured Streaming versus Spark Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "683dbd7c5774c93b576f5bbc396dc641",
     "grade": true,
     "grade_id": "answer_3_7",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
